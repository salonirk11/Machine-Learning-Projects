{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Dataset: Predicting POI\n",
    "\n",
    "This project on the infamous Enron email dataset involves predicting if an employee was a 'Person of Interest' during the Enron scam investigation.\n",
    "\n",
    "The dataset is availabe on https://www.cs.cmu.edu/~./enron/ .\n",
    "\n",
    "\n",
    "### Importing necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "The function 'get_nan_counts' converts 'NaN' string to np.nan returning a pandas dataframe of each feature and it's corresponding percent null values (nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nan_counts(dictionary):\n",
    "    \n",
    "    my_df = pd.DataFrame(dictionary).transpose()\n",
    "    nan_counts_dict = {}\n",
    "    for column in my_df.columns:\n",
    "        my_df[column] = my_df[column].replace('NaN',np.nan)\n",
    "        nan_counts = my_df[column].isnull().sum()\n",
    "        nan_counts_dict[column] = round(float(nan_counts)/float(len(my_df[column])) * 100,1)\n",
    "    df = pd.DataFrame(nan_counts_dict,index = ['percent_nan']).transpose()\n",
    "    df.reset_index(level=0,inplace=True)\n",
    "    df = df.rename(columns = {'index':'feature'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Select what features to use. Initially we use the maximum possible features.\n",
    "We use k-best to find the importance of each feature for training. Using a lot of unnecessary features can lead us to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = ['poi', 'salary', 'deferral_payments','total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'other', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'director_fees', 'shared_receipt_with_poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dictionary\n",
    "\n",
    "Load the dictionary containing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOTAL', 26704229)\n",
      "('SKILLING JEFFREY K', 1111258)\n",
      "('LAY KENNETH L', 1072321)\n",
      "('FREVERT MARK A', 1060932)\n",
      "('BHATNAGAR SANJAY', 888)\n"
     ]
    }
   ],
   "source": [
    "outliers = []\n",
    "for key in data_dict:\n",
    "    val = data_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    outliers.append((key,int(val)))\n",
    "\n",
    "outliers = sorted(outliers,key=lambda x:x[1],reverse=True)[:4]\n",
    "outliers.append(('BHATNAGAR SANJAY', 888))\n",
    "for x in outliers:\n",
    "    print(x)\n",
    "    data_dict.pop(x[0],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new features\n",
    "\n",
    "Visualizing the data from the dataset, we see that there are features which when combined with other features are of importance to us. \n",
    "The ratio between the mails sent to poi and the ones sent to all will tell us how often the person communicates with the poi. Similarly, The ratio between the mails recieved from poi and the received mails will be an interesting feature too. We will see further that the fraction of mails received from poi are not much of a deciding feature.\n",
    "\n",
    "We create two new features: 'fraction_from_poi_email' and 'fraction_to_poi_email' and them to the daatset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dataset = data_dict\n",
    "\n",
    "def dict_to_list(key,normalizer):\n",
    "    new_list=[]\n",
    "\n",
    "    for i in data_dict:\n",
    "        if data_dict[i][key]==\"NaN\" or data_dict[i][normalizer]==\"NaN\":\n",
    "            new_list.append(0.)\n",
    "        elif data_dict[i][key]>=0:\n",
    "            new_list.append(float(data_dict[i][key])/float(data_dict[i][normalizer]))\n",
    "    return new_list\n",
    "\n",
    "fraction_from_poi_email=dict_to_list(\"from_poi_to_this_person\",\"to_messages\")\n",
    "fraction_to_poi_email=dict_to_list(\"from_this_person_to_poi\",\"from_messages\")\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in data_dict:\n",
    "    data_dict[i][\"fraction_from_poi_email\"] = fraction_from_poi_email[count]\n",
    "    data_dict[i][\"fraction_to_poi_email\"] = fraction_to_poi_email[count]\n",
    "    count += 1\n",
    "\n",
    "my_dataset = data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the best features\n",
    "\n",
    "We select the four best features using the k-best method. After analysing the importance scores, we decide a new features_list. The selected features list will be best suited for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.28161841   0.05748658   2.55178656   0.128223     8.56202764\n",
      "   0.74093048  18.01617966  10.92757014   5.42330897   0.51551682\n",
      "  11.73424577   3.37327549   1.14246906   1.76254073   6.33964132]\n"
     ]
    }
   ],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "k=4\n",
    "k_best = SelectKBest(k=k)\n",
    "k_best.fit(features, labels)\n",
    "scores = k_best.scores_\n",
    "print(scores)\n",
    "\n",
    "features_list = ['poi','total_stock_value','fraction_to_poi_email','expenses','shared_receipt_with_poi']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "featureFormat reads dataset as a dictionary and returns a numpy array of the formatted data.\n",
    "targetFeatureSplit splits data as features and labels for training.\n",
    "\n",
    "These functions are imported from feature_format.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Use the StandardScaler() to scale the values of all the features in features_list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/saloni/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(features_list)-1):\n",
    "    tmp =[]\n",
    "    k=0\n",
    "    for x in features:\n",
    "        tmp.append(float(x[i]))\n",
    "    tmp = StandardScaler().fit_transform(tmp)\n",
    "    for x in features:\n",
    "        x[i]=tmp[k]\n",
    "        k = k + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best classsifier\n",
    "\n",
    "The modified dataset is now trained on a Decision Tree Classifier which is tuned for maximum performance.\n",
    "This classifier is tested for performance using a tester function defined in tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=20, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=1, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.88343\tPrecision: 0.60575\tRecall: 0.52700\tF1: 0.56364\tF2: 0.54107\n",
      "\tTotal predictions: 14000\tTrue positives: 1054\tFalse positives:  686\tFalse negatives:  946\tTrue negatives: 11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
    "            max_features=None, max_leaf_nodes=20, min_impurity_split=1e-07,\n",
    "            min_samples_leaf=1, min_samples_split=5,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\n",
    "test_classifier(clf,my_dataset,features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Use cross_validation.train_test_split to get validation data from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recall\n",
    "\n",
    "The recall_score is one of the many methods of evaluating the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(labels_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
